{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# DAT257x: Reinforcement Learning Explained\n\n## Lab 7: Policy Gradient\n\n### Exercise 7.3: Actor Critic"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Objectives\n* Implement A3C-like N-step updating: $Q(s_t,a_t) = E[r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^n V(s_{t+n})]$\n* Play around with different values of $n$. How does the value of $n$ affect the variance and performance of the algorithm?\n\n## Success Criterion\nThe variance with n-step updates should be even smaller than that of Baselined Reinforce. A correct implementation will not solve the Cartpole domain faster or more frequently, but it should do so with less variance. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import cntk as C\nfrom cntk.layers import Sequential, Dense\nfrom cntk.logging import ProgressPrinter\nimport numpy as np\n\nimport sys\nif \"../\" not in sys.path:\n    sys.path.append(\"../\") \n    \nimport gym\nfrom lib.running_variance import RunningVariance\nfrom lib import plotting\n\nnp.random.seed(123)\nC.cntk_py.set_fixed_random_seed(123)\nC.cntk_py.force_deterministic_algorithms()\n\nenv = gym.make('CartPole-v0')\n\nstate_dim = env.observation_space.shape[0] # Dimension of state space\naction_count = env.action_space.n # Number of actions\nhidden_size = 128 # Number of hidden units\nupdate_frequency = 20\n\n# The policy network maps an observation to a probability of taking action 0 or 1.\nobservations = C.sequence.input_variable(state_dim, np.float32, name=\"obs\")\nW1 = C.parameter(shape=(state_dim, hidden_size), init=C.glorot_uniform(), name=\"W1\")\nb1 = C.parameter(shape=hidden_size, name=\"b1\")\nlayer1 = C.relu(C.times(observations, W1) + b1)\nW2 = C.parameter(shape=(hidden_size, action_count), init=C.glorot_uniform(), name=\"W2\")\nb2 = C.parameter(shape=action_count, name=\"b2\")\nlayer2 = C.times(layer1, W2) + b2\noutput = C.sigmoid(layer2, name=\"output\")\n\n# Label will tell the network what action it should have taken.\nlabel = C.sequence.input_variable(1, np.float32, name=\"label\")\n# return_weight is a scalar containing the discounted return. It will scale the PG loss.\nreturn_weight = C.sequence.input_variable(1, np.float32, name=\"weight\")\n# PG Loss \nloss = -C.reduce_mean(C.log(C.square(label - output) + 1e-4) * return_weight, axis=0, name='loss')\n\n# Build the optimizer\nlr_schedule = C.learning_rate_schedule(lr=0.1, unit=C.UnitType.sample) \nm_schedule = C.momentum_schedule(0.99)\nvm_schedule = C.momentum_schedule(0.999)\noptimizer = C.adam([W1, W2], lr_schedule, momentum=m_schedule, variance_momentum=vm_schedule)\n\n# Create a buffer to manually accumulate gradients\ngradBuffer = dict((var.name, np.zeros(shape=var.shape)) for var in loss.parameters if var.name in ['W1', 'W2', 'b1', 'b2'])\n\n# Define the critic network\ncritic = Sequential([\n    Dense(128, activation=C.relu, init=C.glorot_uniform()),\n    Dense(1, activation=None, init=C.glorot_uniform(scale=.01))\n])(observations)\n\n# Define target and Squared Error Loss Function, adam optimizier, and trainer for the Critic.\ncritic_target = C.sequence.input_variable(1, np.float32, name=\"target\")\ncritic_loss = C.squared_error(critic, critic_target)\ncritic_lr_schedule = C.learning_rate_schedule(lr=0.1, unit=C.UnitType.sample) \ncritic_optimizer = C.adam(critic.parameters, critic_lr_schedule, momentum=m_schedule, variance_momentum=vm_schedule)\ncritic_trainer = C.Trainer(critic, (critic_loss, None), critic_optimizer)\n\ndef discount_rewards(r, gamma=0.999):\n    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    for t in reversed(range(0, r.size)):\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you need to implement the function that computes n-step update targets:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO: Create a function that returns an array of n-step targets, one for each timestep:\n# target[t] = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... + \\gamma^n V(s_{t+n})\n# Where r_t is given by episode reward (epr) and V(s_n) is given by the baselines.\ndef compute_n_step_targets(epr, baselines, gamma=0.999, n=15):\n    \"\"\" Computes a n_step target value. \"\"\"\n    n_step_targets = np.zeros_like(epr)\n\n    ## Code here\n    \n    return n_step_targets",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The main loop is the same and should not need modification except for trying different values of $n$."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "running_variance = RunningVariance()\nreward_sum = 0\n\nmax_number_of_episodes = 500\n\nstats = plotting.EpisodeStats(\n    episode_lengths=np.zeros(max_number_of_episodes),\n    episode_rewards=np.zeros(max_number_of_episodes),\n    episode_running_variance=np.zeros(max_number_of_episodes))\n\nfor episode_number in range(max_number_of_episodes):\n    states, rewards, labels = [],[],[]\n    done = False\n    observation = env.reset()\n    t = 1\n    while not done:\n        state = np.reshape(observation, [1, state_dim]).astype(np.float32)\n        states.append(state)\n\n        # Run the policy network and get an action to take.\n        prob = output.eval(arguments={observations: state})[0][0][0]\n        # Sample from the bernoulli output distribution to get a discrete action\n        action = 1 if np.random.uniform() < prob else 0\n\n        # Pseudo labels to encourage the network to increase\n        # the probability of the chosen action. This label will be used\n        # in the loss function above.\n        y = 1 if action == 0 else 0  # a \"fake label\"\n        labels.append(y)\n\n        # step the environment and get new measurements\n        observation, reward, done, _ = env.step(action)\n        reward_sum += float(reward)\n\n        # Record reward (has to be done after we call step() to get reward for previous action)\n        rewards.append(float(reward))\n        \n        stats.episode_rewards[episode_number] += reward\n        stats.episode_lengths[episode_number] = t\n        t += 1\n\n    # Stack together all inputs, hidden states, action gradients, and rewards for this episode\n    epx = np.vstack(states)\n    epl = np.vstack(labels).astype(np.float32)\n    epr = np.vstack(rewards).astype(np.float32)\n\n    # Compute the discounted reward backwards through time.\n    discounted_epr = discount_rewards(epr)\n\n    # Train the critic to predict the discounted reward from the observation\n    critic_trainer.train_minibatch({observations: epx, critic_target: discounted_epr})\n    baseline = critic.eval({observations: epx})\n    \n    # Compute n-step targets\n    n_step_targets = compute_n_step_targets(epr, baseline[0])\n\n    # Compute the baselined returns: A = n_step_targets - b(s). Weight the gradients by this value.\n    baselined_returns = n_step_targets - baseline\n    \n    # Keep a running estimate over the variance of of the discounted rewards\n    for r in baselined_returns:\n        running_variance.add(r[0,0])\n\n    # Forward pass\n    arguments = {observations: epx, label: epl, return_weight: baselined_returns}\n    state, outputs_map = loss.forward(arguments, outputs=loss.outputs,\n                                      keep_for_backward=loss.outputs)\n\n    # Backward pass\n    root_gradients = {v: np.ones_like(o) for v, o in outputs_map.items()}\n    vargrads_map = loss.backward(state, root_gradients, variables=set([W1, W2]))\n\n    for var, grad in vargrads_map.items():\n        gradBuffer[var.name] += grad\n\n    # Only update every 20 episodes to reduce noise\n    if episode_number % update_frequency == 0:\n        grads = {W1: gradBuffer['W1'].astype(np.float32),\n                 W2: gradBuffer['W2'].astype(np.float32)}\n        updated = optimizer.update(grads, update_frequency)\n\n        # reset the gradBuffer\n        gradBuffer = dict((var.name, np.zeros(shape=var.shape))\n                          for var in loss.parameters if var.name in ['W1', 'W2', 'b1', 'b2'])\n\n        print('Episode: %d. Average reward for episode %f. Variance %f' % (episode_number, reward_sum / update_frequency, running_variance.get_variance()))\n\n        reward_sum = 0\n        \n    stats.episode_running_variance[episode_number] = running_variance.get_variance()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "plotting.plot_pgresults(stats)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}